ğŸ› ï¸ Real-Time Data Engineering Pipeline

This project builds a real-time data engineering pipeline that streams data using Apache Kafka, processes it using Apache Spark, stores it in PostgreSQL, and optionally integrates with AWS S3 for long-term storage. It's orchestrated with Apache Airflow for production-grade scheduling and monitoring.

ğŸ“Œ Features

ğŸ”„ Real-time data ingestion with Apache Kafka

âš¡ Scalable stream processing with Apache Spark

ğŸ—ƒï¸ Data storage and transformation using PostgreSQL

â˜ï¸ Optional S3 integration for raw/processed data backup

ğŸ§© Modular and production-ready codebase

ğŸ› ï¸ Airflow (optional) for job orchestration

ğŸ§± Architecture Overview

Data Source â†’ Kafka â†’ Spark Streaming â†’ PostgreSQL
                            â†“
                         (Optional) S3

ğŸ–¥ï¸ Prerequisites

Install the following:

Python 3.8+

Apache Kafka

Apache Spark

PostgreSQL or Docker

AWS CLI (optional)

Apache Airflow (optional)

ğŸš€ Getting Started

1. Clone the Repo

git clone https://github.com/yourusername/data-pipeline.git

cd data-pipeline

2. Install Python Dependencies

pip install -r requirements.txt

3. Start Kafka Services

Terminal 1: Start Zookeeper

bin/zookeeper-server-start.sh config/zookeeper.properties

Terminal 2: Start Kafka Broker

bin/kafka-server-start.sh config/server.properties

4. Run Spark Streaming Job

spark-submit spark/stream_processor.py

5. Start PostgreSQL

OR use Docker

docker run -d -p 5432:5432 --name pg -e POSTGRES_PASSWORD=admin postgres

6. Optional: AWS Integration

aws configure

# Add your S3 bucket settings in config.py

7. Optional: Launch Airflow

cd airflow/

docker-compose up airflow-init

docker-compose up

ğŸ§ª Sample Kafka Producer

python kafka_producer/produce_mock_data.py

ğŸ“Š Output

Live processed data stored in PostgreSQL

Historical/archived data optionally pushed to AWS S3

Visualize or query from PostgreSQL using pgAdmin or CLI

ğŸ“‚ Project Structure

data-pipeline/

â”œâ”€â”€ kafka_producer/

â”œâ”€â”€ spark/

â”œâ”€â”€ airflow/

â”œâ”€â”€ utils/

â”œâ”€â”€ config.py

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

ğŸ§  Challenges Faced

Kafkaâ€“Spark integration & schema compatibility

Spark checkpointing and fault tolerance setup

PostgreSQL performance tuning for bulk inserts

Docker networking between services

Managing retries, failures, and stateful stream apps

ğŸ“Œ Tech Stack

Python

Apache Kafka

Apache Spark (Structured Streaming)

PostgreSQL

Airflow (Optional)

AWS S3 (Optional)

Docker (Recommended for local orchestration)

ğŸ¤ Contributions

Feel free to fork, improve, or raise issues!






